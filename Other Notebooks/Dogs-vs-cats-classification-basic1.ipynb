{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 24446 arrays: [array([[1],\n       [0]]), array([[1],\n       [0]]), array([[1],\n       [0]]), array([[1],\n       [0]]), array([[0],\n       [1]]), array([[1],\n       [0]]), array([[0],\n       [1]]), array([[0],\n     ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6af47f97d755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m           \u001b[0;31m#validation_data=(test_x, test_y),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m           callbacks=[tensorboard])\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programming/MachineLearningVirtualEnv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programming/MachineLearningVirtualEnv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2419\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2421\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programming/MachineLearningVirtualEnv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    321\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 24446 arrays: [array([[1],\n       [0]]), array([[1],\n       [0]]), array([[1],\n       [0]]), array([[1],\n       [0]]), array([[0],\n       [1]]), array([[1],\n       [0]]), array([[0],\n       [1]]), array([[0],\n     ..."
     ]
    }
   ],
   "source": [
    "# for tensorboard log folder name\n",
    "from time import time\n",
    "\n",
    "# working with, mainly resizing, images\n",
    "import cv2                 \n",
    "\n",
    "# dealing with arrays\n",
    "import numpy as np         \n",
    "\n",
    "# dealing with directories\n",
    "import os                  \n",
    "\n",
    "# mixing up or currently ordered data that might lead our network astray in training.\n",
    "from random import shuffle \n",
    "\n",
    "# a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "from tqdm import tqdm      \n",
    "\n",
    "# neural networks\n",
    "#import tflearn\n",
    "#from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "#from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "#from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "# use PlaidML as backend for hardware acceleration\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "TRAIN_DIR = './train'\n",
    "TEST_DIR = './test'\n",
    "IMG_SIZE = 64\n",
    "#LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# just so we remember which saved model is which, sizes must match\n",
    "MODEL_NAME = './dogs-vs-cats-{}.model'.format('2conv-basic1') \n",
    "\n",
    "def label_img(imgname):\n",
    "    file_extension = imgname.split('.')[-1]\n",
    "    if file_extension != 'jpg':\n",
    "        return []\n",
    "    word_label = imgname.split('.')[-3]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    #                            [much cat, no dog]\n",
    "    if word_label == 'cat': return [1,0]\n",
    "    #                             [no cat, very doggo]\n",
    "    elif word_label == 'dog': return [0,1]\n",
    "\n",
    "    \n",
    "def create_train_data():\n",
    "    training_data = []\n",
    "    img = []\n",
    "    for imgname in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        label = label_img(imgname)\n",
    "        if label == []:\n",
    "            continue\n",
    "        try:\n",
    "            path = os.path.join(TRAIN_DIR,imgname)\n",
    "            img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "            training_data.append([np.array(img),np.array(label)])\n",
    "        except Exception as e:\n",
    "            print(\"error with image: '{}', exception: {}\".format(imgname, str(e)))\n",
    "    shuffle(training_data)\n",
    "    np.save('./train_data_64.npy', training_data)\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def create_test_data():\n",
    "    testing_data = []\n",
    "    for imgname in tqdm(os.listdir(TEST_DIR)):\n",
    "        try:\n",
    "            path = os.path.join(TEST_DIR,imgname)\n",
    "            img_num = imgname.split('.')[0]\n",
    "            img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "            testing_data.append([np.array(img), img_num])\n",
    "        except Exception as e:\n",
    "            print(\"error with image: '{}', exception: {}\".format(imgname, str(e)))\n",
    "    shuffle(testing_data)\n",
    "    np.save('./test_data_64.npy', testing_data)\n",
    "    return testing_data\n",
    "\n",
    "\n",
    "# Check if training data file alrady exists\n",
    "train_data_exists = os.path.isfile('./train_data_64.npy')\n",
    "if train_data_exists:\n",
    "    # We have already created the dataset\n",
    "    train_data = np.load('./train_data_64.npy')\n",
    "else:\n",
    "    # Create the dataset\n",
    "    train_data = create_train_data()\n",
    "\n",
    "# Check if testing data file alrady exists\n",
    "test_data_exists = os.path.isfile('./test_data_64.npy')\n",
    "if test_data_exists:\n",
    "    # We have already created the dataset\n",
    "    test_data = np.load('./test_data_64.npy')\n",
    "else:\n",
    "    # Create the dataset\n",
    "    test_data = create_test_data()\n",
    "\n",
    "    \n",
    "    \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu, input_shape=(64, 64, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2,  activation=tf.nn.softmax)\n",
    "])\n",
    "    \n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if os.path.exists('./{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "    \n",
    "train = train_data[:-500]\n",
    "test = train_data[-500:]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1] for i in test]\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "#model.fit(\n",
    "#    {'input': X}, \n",
    "#    {'targets': Y}, \n",
    "#    n_epoch=1, \n",
    "#    validation_set=(\n",
    "#        {'input': test_x}, \n",
    "#        {'targets': test_y}), \n",
    "#    snapshot_step=500, \n",
    "#    show_metric=True, \n",
    "#    run_id=MODEL_NAME,\n",
    "#    callbacks=[tensorboard])\n",
    "\n",
    "model.fit(X, \n",
    "          Y, \n",
    "          epochs=1, \n",
    "          steps_per_epoch=500,\n",
    "          validation_split=0.1,\n",
    "          #validation_data=(test_x, test_y),\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "\n",
    "model.save(MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
